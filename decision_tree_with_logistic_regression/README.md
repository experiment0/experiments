# Дерево решений с использованием логистической регрессии в качестве разделяющей плоскости

## Оглавление

[Описание проекта](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Описание-проекта)\
[Краткая информация о данных](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Краткая-информация-о-данных)\
[Этапы работы над проектом](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Этапы-работы-над-проектом)\
[Описание файлов](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Описание-файлов)\
[Результат](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Результат)

## Описание проекта

Для закрепления понимания работы алгоритма дерева решений он был реализован вручную.\
Отдельные функции для его реализации были даны в курсе [Профессия Data Scientist](https://skillfactory.ru/data-scientist-pro).\
Возникло желание собрать их в класс, прописать типы и добавить комментарии.\
В процессе реализации возникла идея взять в качестве разделяющей плоскости не предикат, а логистическую регрессию.\
Было предположение, взвешенная неоднородность после разделения с помощью логистической регрессии будет меньше, чем с помощью разделения по предикату.\
А это в свою очередь даст лучший результат на каждом шаге и как следствие на выходе.\
После реализации эксперементальной модели проведено ее более подробное исследование и сделан вывод о том, что логистическая регрессия не подходит как предикат для разделения выборки в дереве решений.\
Дано объяснение, почему.

## Краткая информация о данных

В качестве тестовых данных взяты таблицы из библиотеки `sklearn.datasets.fetch_openml`.\
Полный набор данных, который может предоставить библиотека, можно найти [здесь](https://openml.org/search?type=data&sort=runs&status=active).\
Были выбраны данные для регрессии и классификации.\

Для более полного исследования эксперементальной модели были использованы  [наборы данных для бинарной классификации, представленные на kaggle](https://www.kaggle.com/datasets?tags=14201-Binary+Classification).

Признаки используемых таблиц описаны в основном файле проекта [./index.ipynb](./index.ipynb)

## Этапы работы над проектом

- В файле `DecisionTree.py` вручную реализован алгоритм дерева решений для регрессии и классификации.
- Проведено сравнение результатов его работы с классами `DecisionTreeRegressor` и `DecisionTreeClassifier` из библиотеки `sklearn.tree`.
- В файле `DecisionTreeWithLogisticRegression.py` реализован аналогичный алгоритм дерева решений для бинарной классификации,\
  но в качестве плоскости, разделяющей данные на 2 выборки на каждом шаге, взят не предикат, \
  а логистическая регрессия.
- Проведено сравнение результатов созданного алгоритма и библиотек из `sklearn.tree`.\
  А также проведено сравнение взвешенной неоднородности после разделения выборок на первом шаге для обоих алгоритмов классификации (классического и эксперементального).
- Далее проведено исследование эксперементальной модели на разных наборах данных.\
Сделан вывод и дано объяснение, почему логистическая регрессия не подходит в качестве предиката для разделения выборок дерева решений на каждом шаге.

## Описание файлов

- [./index.ipynb](./index.ipynb) - основной файл проекта, содержит описание хода исследования.
- [./classes/DecisionTree.py](./classes/DecisionTree.py) - содержит код класса для реализации дерева решений вручную.
- [./classes/DecisionTreeWithLogisticRegression.py](./classes/DecisionTreeWithLogisticRegression.py) - содержит код класса для реализации эксперементального алгоритма дерева решений для бинарной классификации, где в качестве разделяющей плоскости на каждом шаге используется логистическая регрессия.
- [./helpers/criterions.py](./helpers/criterions.py) - содержит вспомогательные функции для расчета критерия информативности.
- [./check.ipynb](./check.ipynb) - содержит исследование эксперементальной модели на разных наборах данных и итоговый вывод.

## Результат

В результате эксперимента сделан вывод о том, что логистическая регрессия не подходит в качестве предиката для разделения выборок дерева решений на каждом шаге.\
Потому что в результате деления выборок с помощью логистической регрессии мы довольно быстро приходим к тому, что данные в этих выборках становятся линейно не разделимыми и деление прекращается.\
Метркики качества деления соответственно перестают расти с увеличением глубины дерева.

В результате исследования получено более глубокое понимание работы моделей.

:arrow_up:[к оглавлению](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Оглавление)
