# Дерево решений с использованием логистической регрессии в качестве разделяющей плоскости

## Оглавление

[Описание проекта](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Описание-проекта)\
[Краткая информация о данных](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Краткая-информация-о-данных)\
[Этапы работы над проектом](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Этапы-работы-над-проектом)\
[Описание файлов](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Описание-файлов)\
[Результат](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Результат)

## Описание проекта

Для закрепления понимания работы алгоритма дерева решений он был реализован вручную.\
Отдельные функции для его реализации были даны в курсе [Профессия Data Scientist](https://skillfactory.ru/data-scientist-pro).\
Возникло желание собрать их в класс, прописать типы и добавить комментарии.\
В процессе реализации возникла идея взять в качестве разделяющей плоскости не предикат, а логистическую регрессию.\
Было предположение, взвешенная неоднородность после разделения с помощью логистической регрессии будет меньше,
чем с помощью разделения по предикату.\
А это в свою очередь даст лучший результат на каждом шаге и как следствие на выходе.

## Краткая информация о данных

В качестве тестовых данных взяты таблицы из библиотеки `sklearn.datasets.fetch_openml`.\
Полный набор данных, который может предоставить библиотека, можно найти [здесь](https://openml.org/search?type=data&sort=runs&status=active).\
Были выбраны данные для регрессии и классификации.\
Признаки используемых таблиц описаны в основном файле проекта [./index.ipynb](./index.ipynb)

## Этапы работы над проектом

- В файле `DecisionTree.py` вручную реализован алгоритм дерева решений для регрессии и классификации.
- Проведено сравнение результатов его работы с классами `DecisionTreeRegressor` и `DecisionTreeClassifier` из библиотеки `sklearn.tree`.
- В файле `DecisionTreeWithLogisticRegression.py` реализован аналогичный алгоритм дерева решений для бинарной классификации,\
  но в качестве плоскости, разделяющей данные на 2 выборки на каждом шаге, взят не предикат, \
  а логистическая регрессия.
- Проведено сравнение результатов созданного алгоритма и библиотек из `sklearn.tree`.\
  А также проведено сравнение взвешенной неоднородности после разделения выборок на первом шаге для обоих алгоритмов классификации (классического и эксперементального).
- Сделан вывод о результативности эксперементального алгоритма.

## Описание файлов

- [./index.ipynb](./index.ipynb) - основной файл проекта, содержит описание хода исследования.
- [./classes/DecisionTree.py](./classes/DecisionTree.py) - содержит код класса для реализации дерева решений вручную.
- [./classes/DecisionTreeWithLogisticRegression.py](./classes/DecisionTreeWithLogisticRegression.py) - содержит код класса для реализации эксперементального алгоритма дерева решений для бинарной классификации, где в качестве разделяющей плоскости на каждом шаге используется логистическая регрессия.
- [./helpers/criterions.py](./helpers/criterions.py) - содержит вспомогательные функции для расчета критерия информативности.

## Результат

В результате эксперимента сделан вывод о том, что если в дереве решений для бинарной классификации \
в качестве условия для разделения выборок на каждом шаге \
взять логистическую регрессию вместо предиката,\
то взвешенная неоднородность после деления первой вершины для эксперементального алгоритма будет меньше\
и метрики при 3-х уровневых деревьях для эксперементального алгоритма получились лучше.

Но скорее всего на больших объемах данных данный алгоритм будет использовать слишком накладно.\
По той причине, что для каждого узла дерева нужно будет хранить обученную логистическую регрессию\
и набор предсказаний для каждой вершины.

:arrow_up:[к оглавлению](https://github.com/experiment0/experiments/tree/master/decision_tree_with_logistic_regression#Оглавление)
